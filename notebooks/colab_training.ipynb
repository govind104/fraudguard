{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsjoNAnuj7ws"
      },
      "source": [
        "# üîí FraudGuard Training Notebook\n",
        "\n",
        "**AD-RL-GNN Fraud Detection** | Full training pipeline with mini-batch processing\n",
        "\n",
        "This notebook trains the FraudGuard model on the IEEE-CIS fraud detection dataset using:\n",
        "- **NeighborLoader** for memory-efficient mini-batch training\n",
        "- **FAISS** for similarity graph construction (GPU if available, CPU fallback)\n",
        "- **FocalLoss** for class-imbalanced learning\n",
        "\n",
        "**Target Metrics:**\n",
        "- Specificity: 98.72%\n",
        "- G-Means Improvement: 18.11%\n",
        "- P95 Latency: <100ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJcMLlwrj7wu"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQkR0H15j7wv",
        "outputId": "cf3c2f25-63da-407c-8cf8-6e9e96afc724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive for data storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FiJD_x_j7ww",
        "outputId": "5a90860d-d73d-4bde-ff30-ad1a0c3ddf30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'fraudguard' already exists and is not an empty directory.\n",
            "/content/fraudguard\n"
          ]
        }
      ],
      "source": [
        "# Clone repository\n",
        "!git clone https://github.com/govind104/fraudguard.git\n",
        "%cd fraudguard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vsh_FW9lj7wx",
        "outputId": "89f0b04d-4aae-4dde-98f8-cac6af33d101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è faiss-gpu not available, using faiss-cpu\n",
            "   (Graph building on CPU, but GNN training still runs on GPU!)\n",
            "PyTorch: 2.9.0, CUDA: cu126\n",
            "Downloading from: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.2-cp312-cp312-linux_x86_64.whl\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.18-cp312-cp312-linux_x86_64.whl\n",
            "Collecting scipy (from torch-sparse)\n",
            "  Using cached scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting numpy<2.7,>=1.26.4 (from scipy->torch-sparse)\n",
            "  Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Using cached scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
            "Using cached numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: torch-scatter, numpy, scipy, torch-sparse\n",
            "  Attempting uninstall: torch-scatter\n",
            "    Found existing installation: torch_scatter 2.1.2\n",
            "    Uninstalling torch_scatter-2.1.2:\n",
            "      Successfully uninstalled torch_scatter-2.1.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.17.0\n",
            "    Uninstalling scipy-1.17.0:\n",
            "      Successfully uninstalled scipy-1.17.0\n",
            "  Attempting uninstall: torch-sparse\n",
            "    Found existing installation: torch_sparse 0.6.18\n",
            "    Uninstalling torch_sparse-0.6.18:\n",
            "      Successfully uninstalled torch_sparse-0.6.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fraudguard 0.1.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.4.1 scipy-1.17.0 torch-scatter-2.1.2 torch-sparse-0.6.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c4eb9097499546cd9ccfa35e9fc105d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/fraudguard\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: faiss-cpu<2.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (1.13.2)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from fraudguard==0.1.0)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: structlog<24.0.0,>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (23.3.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torch-geometric<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (2.7.0)\n",
            "Requirement already satisfied: torch-scatter<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (2.1.2)\n",
            "Requirement already satisfied: torch-sparse<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from fraudguard==0.1.0) (0.6.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->fraudguard==0.1.0) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.0.0->fraudguard==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.0.0->fraudguard==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.0.0->fraudguard==0.1.0) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.3.0->fraudguard==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.3.0->fraudguard==0.1.0) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.3.0->fraudguard==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (3.13.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.0->fraudguard==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->fraudguard==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric<3.0.0,>=2.3.0->fraudguard==0.1.0) (2026.1.4)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Building wheels for collected packages: fraudguard\n",
            "  Building editable for fraudguard (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fraudguard: filename=fraudguard-0.1.0-py3-none-any.whl size=2801 sha256=abd00cd285315796bfb55d9faae784f5c5711c89a41847e0d54c9e072446ded1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2bu4zjn6/wheels/c6/29/62/fb6d8d095576e7e3efddf4fdcb7dfc799af71ace273f1ee84c\n",
            "Successfully built fraudguard\n",
            "Installing collected packages: numpy, fraudguard\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.4.1\n",
            "    Uninstalling numpy-2.4.1:\n",
            "      Successfully uninstalled numpy-2.4.1\n",
            "  Attempting uninstall: fraudguard\n",
            "    Found existing installation: fraudguard 0.1.0\n",
            "    Uninstalling fraudguard-0.1.0:\n",
            "      Successfully uninstalled fraudguard-0.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fraudguard-0.1.0 numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bc8037f0d7d94a11b5493cf79f0c395e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# Note: faiss-gpu may not be available on Python 3.12\n",
        "# The code will fallback to faiss-cpu automatically\n",
        "# GNN training STILL runs on GPU - only graph building uses CPU FAISS\n",
        "!pip install -q torch torch-geometric pandas numpy scikit-learn pyyaml structlog\n",
        "\n",
        "# Try faiss-gpu first, fallback to faiss-cpu\n",
        "import subprocess\n",
        "result = subprocess.run(['pip', 'install', '-q', 'faiss-gpu'], capture_output=True)\n",
        "if result.returncode != 0:\n",
        "    print('‚ö†Ô∏è faiss-gpu not available, using faiss-cpu')\n",
        "    print('   (Graph building on CPU, but GNN training still runs on GPU!)')\n",
        "    !pip install -q faiss-cpu\n",
        "else:\n",
        "    print('‚úì faiss-gpu installed')\n",
        "\n",
        "import torch\n",
        "\n",
        "# 1. Get exact versions\n",
        "pt_version = torch.__version__.split('+')[0]  # e.g., 2.5.1\n",
        "cuda_version = \"cu\" + torch.version.cuda.replace('.', '')  # e.g., cu124\n",
        "wheel_url = f\"https://data.pyg.org/whl/torch-{pt_version}+{cuda_version}.html\"\n",
        "\n",
        "print(f\"PyTorch: {pt_version}, CUDA: {cuda_version}\")\n",
        "print(f\"Downloading from: {wheel_url}\")\n",
        "\n",
        "# 2. Install with visible output (force reinstall to fix broken partial installs)\n",
        "!pip install --force-reinstall torch-scatter torch-sparse -f $wheel_url\n",
        "\n",
        "# Install repo in editable mode\n",
        "!pip install -e .\n",
        "\n",
        "print('\\n‚úì Environment setup complete')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "    import torch_scatter\n",
        "    import torch_sparse\n",
        "    import fraudguard\n",
        "    print(\"‚úÖ Success! Libraries are installed and loaded.\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Still missing libraries: {e}\")\n",
        "    # Only if you see this error should you go back and install again."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32LXcfp8_bZ8",
        "outputId": "4316a4f3-0b10-421a-8b70-e2c81d607619"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success! Libraries are installed and loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQIB4vEej7wx"
      },
      "source": [
        "## 2Ô∏è‚É£ Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQDtmVWBj7wy",
        "outputId": "96a1dbca-022a-43b5-b76f-c9ae1ed822fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: /content/drive/MyDrive/ieee-fraud-detection\n",
            "Models: /content/drive/MyDrive/fraudguard-models\n",
            "Logs: /content/drive/MyDrive/fraudguard-logs\n",
            "\n",
            "Batch size: 4096\n",
            "Sample fraction: 100%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ==============================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS AS NEEDED\n",
        "# ==============================================\n",
        "\n",
        "# Data paths - Point to your Google Drive folders\n",
        "DATA_DIR = \"/content/drive/MyDrive/ieee-fraud-detection\"\n",
        "MODELS_DIR = \"/content/drive/MyDrive/fraudguard-models\"\n",
        "LOGS_DIR = \"/content/drive/MyDrive/fraudguard-logs\"\n",
        "\n",
        "# Training parameters\n",
        "SAMPLE_FRAC = 1.0      # Use full dataset (1.0 = 100%)\n",
        "MAX_EPOCHS = 30\n",
        "BATCH_SIZE = 4096      # Reduce to 2048 or 1024 if OOM\n",
        "NUM_NEIGHBORS = [25, 10]  # 2-hop neighborhood sampling\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Data: {DATA_DIR}\")\n",
        "print(f\"Models: {MODELS_DIR}\")\n",
        "print(f\"Logs: {LOGS_DIR}\")\n",
        "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
        "print(f\"Sample fraction: {SAMPLE_FRAC*100:.0f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM3sXum3j7wz"
      },
      "source": [
        "## 3Ô∏è‚É£ Verify GPU and FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7CHUJ3nj7wz",
        "outputId": "3d08a000-d170-4f81-b3d5-66254dfe1360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.8 GB\n",
            "\n",
            "‚úì GNN training will run on GPU\n",
            "\n",
            "FAISS GPUs: 0\n",
            "   (Using CPU FAISS for graph building - this is OK)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import faiss\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(\"\\n‚úì GNN training will run on GPU\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: No GPU detected. Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "# Check FAISS GPU\n",
        "faiss_gpus = faiss.get_num_gpus() if hasattr(faiss, 'get_num_gpus') else 0\n",
        "print(f\"\\nFAISS GPUs: {faiss_gpus}\")\n",
        "if faiss_gpus == 0:\n",
        "    print(\"   (Using CPU FAISS for graph building - this is OK)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmk_r0omj7w0"
      },
      "source": [
        "## 4Ô∏è‚É£ Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVU_fGbPj7w0",
        "outputId": "746c569a-ce60-4d2f-e769-e2f21a564d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading faiss with CPU support (no GPU detected).\n",
            "Using device: cuda\n",
            "\n",
            "Data loaded:\n",
            "  Train: 354,324\n",
            "  Val: 118,108\n",
            "  Test: 118,108\n",
            "  Fraud rate: 3.50%\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/fraudguard')\n",
        "\n",
        "from pathlib import Path\n",
        "from src.data.loader import FraudDataLoader\n",
        "from src.data.preprocessor import FeaturePreprocessor\n",
        "from src.data.graph_builder import GraphBuilder\n",
        "from src.utils.config import load_data_config\n",
        "from src.utils.device_utils import set_seed, get_device\n",
        "\n",
        "set_seed(42)\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load config and override path with notebook variable\n",
        "data_cfg = load_data_config()\n",
        "data_cfg.paths.raw_data_dir = Path(DATA_DIR)\n",
        "\n",
        "# Load data with corrected path\n",
        "loader = FraudDataLoader(config=data_cfg)\n",
        "df = loader.load_train_data(sample_frac=SAMPLE_FRAC)\n",
        "train_df, val_df, test_df = loader.create_splits(df)\n",
        "\n",
        "print(f\"\\nData loaded:\")\n",
        "print(f\"  Train: {len(train_df):,}\")\n",
        "print(f\"  Val: {len(val_df):,}\")\n",
        "print(f\"  Test: {len(test_df):,}\")\n",
        "print(f\"  Fraud rate: {df['isFraud'].mean()*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DWymvWAj7w0"
      },
      "source": [
        "## 5Ô∏è‚É£ Build or Load Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsEBjtW5j7w0",
        "outputId": "1b931781-3b20-490c-cc50-54b0143618fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è Preprocessing features to create X_full...\n",
            "‚úì Feature Matrix created: torch.Size([590540, 69])\n",
            "Loading cached graph from /content/drive/MyDrive/fraudguard-models/edges_full.pt...\n",
            "Loaded 28,972,713 edges\n",
            "\n",
            "Final Graph ready on cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "import os\n",
        "from pathlib import Path\n",
        "from src.data.graph_builder import GraphBuilder\n",
        "from src.data.preprocessor import FeaturePreprocessor\n",
        "from src.utils.config import load_model_config, load_data_config\n",
        "\n",
        "# =======================================================\n",
        "# 1. MISSING STEP: Preprocess Data to create X_full\n",
        "# =======================================================\n",
        "print(\"‚öôÔ∏è Preprocessing features to create X_full...\")\n",
        "# Initialize Preprocessor\n",
        "preprocessor = FeaturePreprocessor(data_config=load_data_config(), model_config=load_model_config())\n",
        "\n",
        "# Fit on Train, Transform Val/Test\n",
        "X_train = preprocessor.fit_transform(train_df)\n",
        "X_val = preprocessor.transform(val_df)\n",
        "X_test = preprocessor.transform(test_df)\n",
        "\n",
        "# Create the global feature matrix\n",
        "X_full = torch.cat([X_train, X_val, X_test])\n",
        "print(f\"‚úì Feature Matrix created: {X_full.shape}\")\n",
        "\n",
        "# =======================================================\n",
        "# 2. Load or Build Graph\n",
        "# =======================================================\n",
        "GRAPH_CACHE = f\"{MODELS_DIR}/edges_full.pt\"\n",
        "\n",
        "if os.path.exists(GRAPH_CACHE):\n",
        "    print(f\"Loading cached graph from {GRAPH_CACHE}...\")\n",
        "    edge_index = torch.load(GRAPH_CACHE)\n",
        "    print(f\"Loaded {edge_index.shape[1]:,} edges\")\n",
        "\n",
        "    # Move to device\n",
        "    edge_index = edge_index.to(device)\n",
        "    X_full = X_full.to(device)\n",
        "else:\n",
        "    print(\"üöÄ Starting Memory-Optimized Graph Build (Directed)...\")\n",
        "\n",
        "    # Configure GraphBuilder\n",
        "    model_cfg = load_model_config()\n",
        "    model_cfg.graph.similarity_threshold = 0.90\n",
        "    model_cfg.graph.max_neighbors = 50\n",
        "    model_cfg.graph.batch_size = 50000\n",
        "\n",
        "    builder = GraphBuilder(config=model_cfg)\n",
        "\n",
        "    # Note: We use the tensors we just created above\n",
        "    # Train -> Train\n",
        "    print(\"  Phase 1: Train -> Train...\")\n",
        "    builder.fit(X_train)\n",
        "\n",
        "    # Val/Test -> Train\n",
        "    print(\"  Phase 2: Val/Test -> Train...\")\n",
        "    # Concatenate Val and Test for the transform step\n",
        "    X_val_test = torch.cat([X_val, X_test])\n",
        "\n",
        "    # Use the length of X_train (n_train) to ensure correct indexing\n",
        "    n_train = len(X_train)\n",
        "    edge_index = builder.transform(X_val_test, train_size=n_train)\n",
        "\n",
        "    # Verify\n",
        "    builder.verify_no_leakage(edge_index, train_size=n_train)\n",
        "\n",
        "    # Save\n",
        "    torch.save(edge_index, GRAPH_CACHE)\n",
        "    print(f\"‚úì Saved to {GRAPH_CACHE}\")\n",
        "\n",
        "    # Cleanup builder to free RAM\n",
        "    del builder\n",
        "    gc.collect()\n",
        "\n",
        "    # Move to device\n",
        "    X_full = X_full.to(device)\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "print(f\"\\nFinal Graph ready on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkJbfMUj7w1"
      },
      "source": [
        "## 6Ô∏è‚É£ Setup Mini-Batch Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUs8d8XXj7w1",
        "outputId": "80202f4b-64a2-4a8b-b795-b161e577e714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading data to extract labels...\n",
            "Extracting labels...\n",
            "Dataframes deleted to free RAM.\n",
            "Masks created: Train=354324, Val=118108, Test=118108\n",
            "Initializing NeighborLoaders (Batch Size: 4096)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/loader/neighbor_loader.py:229: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  neighbor_sampler = NeighborSampler(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Train batches: 87\n",
            "‚úì Val batches: 29\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.data import Data\n",
        "from pathlib import Path\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "# 1. Reload Data briefly to get Labels & Lengths (since we deleted them)\n",
        "# We need to re-import loader components if they were lost\n",
        "sys.path.insert(0, '/content/fraudguard')\n",
        "from src.data.loader import FraudDataLoader\n",
        "from src.utils.config import load_data_config\n",
        "\n",
        "print(\"Reloading data to extract labels...\")\n",
        "data_cfg = load_data_config()\n",
        "data_cfg.paths.raw_data_dir = Path(DATA_DIR) # Ensure pointing to Drive\n",
        "loader = FraudDataLoader(config=data_cfg)\n",
        "\n",
        "# Load and split\n",
        "df_temp = loader.load_train_data(sample_frac=SAMPLE_FRAC) # Use same sample_frac!\n",
        "train_df, val_df, test_df = loader.create_splits(df_temp)\n",
        "\n",
        "# 2. Extract Labels & Sizes\n",
        "print(\"Extracting labels...\")\n",
        "train_labels = torch.tensor(train_df[\"isFraud\"].values, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_df[\"isFraud\"].values, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_df[\"isFraud\"].values, dtype=torch.long)\n",
        "\n",
        "n_train = len(train_df)\n",
        "n_val = len(val_df)\n",
        "n_test = len(test_df)\n",
        "\n",
        "# 3. Aggressive Cleanup (Free RAM immediately)\n",
        "del df_temp, train_df, val_df, test_df\n",
        "gc.collect()\n",
        "print(\"Dataframes deleted to free RAM.\")\n",
        "\n",
        "# 4. Prepare Masks & Labels\n",
        "all_labels = torch.cat([train_labels, val_labels, test_labels]).to(device)\n",
        "\n",
        "n_total = n_train + n_val + n_test\n",
        "train_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "val_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "test_mask = torch.zeros(n_total, dtype=torch.bool)\n",
        "\n",
        "# Set masks using the calculated lengths\n",
        "train_mask[:n_train] = True\n",
        "val_mask[n_train : n_train + n_val] = True\n",
        "test_mask[n_train + n_val :] = True\n",
        "\n",
        "print(f\"Masks created: Train={train_mask.sum()}, Val={val_mask.sum()}, Test={test_mask.sum()}\")\n",
        "\n",
        "# 5. Create PyG Data object\n",
        "# Ensure X_full and edge_index are on the correct device\n",
        "if X_full.device != device:\n",
        "    X_full = X_full.to(device)\n",
        "if edge_index.device != device:\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "data = Data(x=X_full, edge_index=edge_index, y=all_labels)\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "# 6. Create NeighborLoaders\n",
        "print(f\"Initializing NeighborLoaders (Batch Size: {BATCH_SIZE})...\")\n",
        "\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=NUM_NEIGHBORS,  # [25, 10]\n",
        "    batch_size=BATCH_SIZE,\n",
        "    input_nodes=train_mask,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=NUM_NEIGHBORS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    input_nodes=val_mask,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train batches: {len(train_loader)}\")\n",
        "print(f\"‚úì Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLiiuijj7w2"
      },
      "source": [
        "## 7Ô∏è‚É£ Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvejByp2j7w2",
        "outputId": "20df22c3-e5d6-4447-cf49-ad79c8d8e580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Applied Balanced Class Weights: [1.0, 30.0]\n",
            "Starting training...\n",
            "\n",
            "Dynamic Patience: 6 epochs\n",
            "Epoch |     Loss |     Spec |   Recall |       F1 |  G-Means\n",
            "-----------------------------------------------------------------\n",
            "    1 |   0.8462 |   29.75% |   72.78% |    7.65% |   46.53%\n"
          ]
        }
      ],
      "source": [
        "from src.models import FraudGNN, FocalLoss, compute_class_weights\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Initialize model\n",
        "model = FraudGNN(in_channels=X_full.shape[1]).to(device)\n",
        "\n",
        "# 1. LOWER LEARNING RATE (Crucial for Stability)\n",
        "# Reduced from 0.01 to 0.001 to prevent the \"Panic\" collapse\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# 2. SET BALANCED WEIGHTS\n",
        "# 15 was too low, 50 was too high.\n",
        "# The natural ratio is ~28. We use 30 to slightly favor recall.\n",
        "print(\"‚öñÔ∏è Applied Balanced Class Weights: [1.0, 30.0]\")\n",
        "weights = torch.tensor([1.0, 30.0]).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "# Training config\n",
        "best_gmeans = 0\n",
        "patience_ratio = 0.20\n",
        "patience = max(5, int(MAX_EPOCHS * patience_ratio))\n",
        "patience_counter = 0\n",
        "history = []\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "print(f\"Dynamic Patience: {patience} epochs\")\n",
        "print(f\"{'Epoch':>5} | {'Loss':>8} | {'Spec':>8} | {'Recall':>8} | {'F1':>8} | {'G-Means':>8}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out[:batch.batch_size], batch.y[:batch.batch_size])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation EVERY EPOCH\n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        all_preds, all_true = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index)\n",
        "                pred = out[:batch.batch_size].argmax(dim=1)\n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_true.extend(batch.y[:batch.batch_size].cpu().numpy())\n",
        "\n",
        "        cm = confusion_matrix(all_true, all_preds, labels=[0, 1])\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        gmeans = np.sqrt(tpr * tnr)\n",
        "        f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "\n",
        "        history.append({'epoch': epoch, 'loss': avg_loss, 'spec': tnr, 'recall': tpr, 'f1': f1, 'gmeans': gmeans})\n",
        "\n",
        "        print(f\"{epoch+1:>5} | {avg_loss:>8.4f} | {tnr*100:>7.2f}% | {tpr*100:>7.2f}% | {f1*100:>7.2f}% | {gmeans*100:>7.2f}%\")\n",
        "\n",
        "        if gmeans > best_gmeans:\n",
        "            best_gmeans = gmeans\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f\"{MODELS_DIR}/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print(f\"\\nTraining complete in {train_time/60:.1f} minutes\")\n",
        "print(f\"Best validation G-Means: {best_gmeans*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgbOgshUj7w2"
      },
      "source": [
        "## 8Ô∏è‚É£ Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsfRXc91j7w2",
        "outputId": "f7bfa309-4998-4a03-eea6-d59100ae7f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/loader/neighbor_loader.py:229: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  neighbor_sampler = NeighborSampler(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINAL TEST RESULTS\n",
            "============================================================\n",
            "\n",
            "Confusion Matrix:\n",
            "  TP: 318  |  FN: 3,746\n",
            "  FP: 7,120  |  TN: 106,924\n",
            "\n",
            "Performance:\n",
            "  Specificity:  93.76%  (CV target: 98.72%)\n",
            "  Recall:       7.82%\n",
            "  F1 Score:     5.53%\n",
            "  G-Means:      27.09%\n",
            "\n",
            "Latency:\n",
            "  Mean: 2.1ms\n",
            "  P95:  2.4ms  (CV target: <100ms)\n",
            "  P99:  2.5ms\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(f\"{MODELS_DIR}/best_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Full neighborhood for test evaluation\n",
        "test_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[-1, -1],  # Full neighborhood\n",
        "    batch_size=BATCH_SIZE,\n",
        "    input_nodes=test_mask,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "all_preds, all_true = [], []\n",
        "latencies = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        start = time.perf_counter()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        latencies.append((time.perf_counter() - start) * 1000)\n",
        "        pred = out[:batch.batch_size].argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_true.extend(batch.y[:batch.batch_size].cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "cm = confusion_matrix(all_true, all_preds, labels=[0, 1])\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "gmeans = np.sqrt(tpr * tnr)\n",
        "f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL TEST RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  TP: {tp:,}  |  FN: {fn:,}\")\n",
        "print(f\"  FP: {fp:,}  |  TN: {tn:,}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Specificity:  {tnr*100:.2f}%  (CV target: 98.72%)\")\n",
        "print(f\"  Recall:       {tpr*100:.2f}%\")\n",
        "print(f\"  F1 Score:     {f1*100:.2f}%\")\n",
        "print(f\"  G-Means:      {gmeans*100:.2f}%\")\n",
        "print(f\"\\nLatency:\")\n",
        "print(f\"  Mean: {np.mean(latencies):.1f}ms\")\n",
        "print(f\"  P95:  {np.percentile(latencies, 95):.1f}ms  (CV target: <100ms)\")\n",
        "print(f\"  P99:  {np.percentile(latencies, 99):.1f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by_t_WxJj7w2"
      },
      "source": [
        "## 9Ô∏è‚É£ Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOd250Gbj7w3",
        "outputId": "220cdf60-c2a6-4434-a024-f2920db9f394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model saved to /content/drive/MyDrive/fraudguard-models/fraudguard_final.pt\n",
            "‚úì Best model saved to /content/drive/MyDrive/fraudguard-models/best_model.pt\n"
          ]
        }
      ],
      "source": [
        "# Save final model with metrics\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"config\": {\n",
        "        \"in_channels\": X_full.shape[1],\n",
        "        \"specificity\": tnr,\n",
        "        \"recall\": tpr,\n",
        "        \"gmeans\": gmeans,\n",
        "        \"f1\": f1,\n",
        "    },\n",
        "    \"history\": history,\n",
        "}, f\"{MODELS_DIR}/fraudguard_final.pt\")\n",
        "\n",
        "print(f\"‚úì Model saved to {MODELS_DIR}/fraudguard_final.pt\")\n",
        "print(f\"‚úì Best model saved to {MODELS_DIR}/best_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IP5wZp1j7w3"
      },
      "source": [
        "## üîü CV Claims Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsfzeerj7w3",
        "outputId": "524017de-efa6-4285-a2c0-2a179eb2b5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CV CLAIMS COMPARISON\n",
            "============================================================\n",
            "| Metric               |     Achieved |     CV Claim | Status |\n",
            "|----------------------|--------------|--------------|--------|\n",
            "| Specificity          |       93.76% |       98.72% |      ‚úó |\n",
            "| P95 Latency          |        2.4ms |       <100ms |      ‚úì |\n",
            "============================================================\n",
            "\n",
            "‚úì = PASS (within tolerance) | ‚úó = INVESTIGATE\n"
          ]
        }
      ],
      "source": [
        "# CV Claims comparison\n",
        "CV_CLAIMS = {\n",
        "    \"specificity\": 98.72,\n",
        "    \"gmeans_improvement\": 18.11,\n",
        "    \"p95_latency_ms\": 100,\n",
        "}\n",
        "\n",
        "achieved_spec = tnr * 100\n",
        "p95_latency = np.percentile(latencies, 95)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CV CLAIMS COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"| {'Metric':<20} | {'Achieved':>12} | {'CV Claim':>12} | {'Status':>6} |\")\n",
        "print(f\"|{'-'*22}|{'-'*14}|{'-'*14}|{'-'*8}|\")\n",
        "\n",
        "# Specificity\n",
        "status_spec = \"‚úì\" if abs(achieved_spec - CV_CLAIMS['specificity']) <= 3 else \"‚úó\"\n",
        "print(f\"| {'Specificity':<20} | {achieved_spec:>11.2f}% | {CV_CLAIMS['specificity']:>11.2f}% | {status_spec:>6} |\")\n",
        "\n",
        "# Latency\n",
        "status_lat = \"‚úì\" if p95_latency < CV_CLAIMS['p95_latency_ms'] else \"‚úó\"\n",
        "print(f\"| {'P95 Latency':<20} | {p95_latency:>10.1f}ms | {'<100':>10}ms | {status_lat:>6} |\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚úì = PASS (within tolerance) | ‚úó = INVESTIGATE\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}